{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "summarizer_latest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameep114/Transformer-networks-for-abstractive-summarization/blob/master/summarizer_latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iByOfYR-xlyj"
      },
      "source": [
        "# !pip install -q --upgrade ipython\n",
        "# !pip install -q --upgrade ipykernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbxhyl_zFlWL"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import unicodedata\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "import csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHmo4QuNtOUk"
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64\n",
        "embedding_dim=300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZku1HbHWH2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cacfc1c-e555-4fb5-b0be-766e36aab19a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "# drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH5cg5pSIHaZ"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_AjGkWXITKA"
      },
      "source": [
        "news = pd.read_csv(\"/content/drive/MyDrive/Datasets/news_summary_more-1 - news_summary_more-1 translated.csv\", delimiter=\",\",encoding=\"UTF8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-rYZhayIe9x"
      },
      "source": [
        "#  news.drop(['headlines','text'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD0X7lBmjM6t"
      },
      "source": [
        " news=news[news.index < 100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXtxc-toIc94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "7ebd1648-bc9f-4358-e33e-3302119a974d"
      },
      "source": [
        "news.tail(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "      <th>HEADLINES</th>\n",
              "      <th>CTEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Decide about an alliance within 15 days: Uddha...</td>\n",
              "      <td>Shiv Sena chief Uddhav Thackeray has asked the...</td>\n",
              "      <td>भाजपा गर्न Uddhav: 15 दिन भित्र एउटा सम्बन्ध ब...</td>\n",
              "      <td>शिवसेना प्रमुख Uddhav ठाकरे 15 दिन भित्रमा शिव...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Grand alliance, if wins, will see a new PM eve...</td>\n",
              "      <td>BJP President Amit Shah on Wednesday took a di...</td>\n",
              "      <td>महान् सम्बन्ध, WINS भने, हरेक दिन नयाँ PM देख्...</td>\n",
              "      <td>बुधबार भाजपा अध्यक्ष अमित शाह \", प्रत्येक विरो...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Demonetisation made homes affordable for youth...</td>\n",
              "      <td>Prime Minister Narendra Modi on Wednesday said...</td>\n",
              "      <td>Demonetisation गरे घरमा युवा को लागि किफायती प...</td>\n",
              "      <td>बुधबार प्रधानमन्त्री नरेन्द्र मोदी आफ्नो सरकार...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headlines  ...                                              CTEXT\n",
              "97  Decide about an alliance within 15 days: Uddha...  ...  शिवसेना प्रमुख Uddhav ठाकरे 15 दिन भित्रमा शिव...\n",
              "98  Grand alliance, if wins, will see a new PM eve...  ...  बुधबार भाजपा अध्यक्ष अमित शाह \", प्रत्येक विरो...\n",
              "99  Demonetisation made homes affordable for youth...  ...  बुधबार प्रधानमन्त्री नरेन्द्र मोदी आफ्नो सरकार...\n",
              "\n",
              "[3 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR2hg9themaN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "51bf34f2-f4bc-45d9-99cc-2c314fc39911"
      },
      "source": [
        "# shuffle garda random value aaidiyera doc val ma kin value rakhya nai tha vayena so remove this\n",
        "from sklearn.utils import shuffle\n",
        "# shuffling the data \n",
        "news = shuffle(news)\n",
        "news.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headlines</th>\n",
              "      <th>text</th>\n",
              "      <th>HEADLINES</th>\n",
              "      <th>CTEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>OnePlus named bestselling premium smartphone b...</td>\n",
              "      <td>OnePlus has emerged as India's best-selling pr...</td>\n",
              "      <td>OnePlus 2018 को प्रिमियम स्मार्टफोन ब्रान्ड बि...</td>\n",
              "      <td>OnePlus मुकाबला गरेर रिपोर्ट प्रति रूपमा, 2018...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
              "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
              "      <td>दिल्ली प्रौद्योगिकी विशेषज्ञ Cred एक वर्षको ला...</td>\n",
              "      <td>Kunal शाह क्रेडिट कार्ड बिल भुक्तानी मंच, Cred...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>Govt to present Interim Budget, not full Budge...</td>\n",
              "      <td>The Finance Ministry on Wednesday said the gov...</td>\n",
              "      <td>फेब्रुअरी 1 मा उपस्थित अन्तरिम बजेट, पूर्ण बजे...</td>\n",
              "      <td>बुधवार मा वित्त मन्त्रालयले स्पष्टीकरण पूर्ण ब...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Isha Ambani features on February cover of Vogu...</td>\n",
              "      <td>Reliance Industries' Chairman Mukesh Ambani's ...</td>\n",
              "      <td>ईशा अंबानी वोग पत्रिका को फेब्रुअरी आवरण मा वि...</td>\n",
              "      <td>रिलायन्स इंडस्ट्रीज 'अध्यक्ष मुकेश अंबानी छोरी...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>Qatar to invest $200 million in Airtel Africa</td>\n",
              "      <td>Airtel on Wednesday said that Qatar's sovereig...</td>\n",
              "      <td>एयरटेल अफ्रीका लगानी गर्न कतार लाख $ 200</td>\n",
              "      <td>बुधबार एयरटेल कतार गरेको सार्वभौम धन कोष कतार ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headlines  ...                                              CTEXT\n",
              "53  OnePlus named bestselling premium smartphone b...  ...  OnePlus मुकाबला गरेर रिपोर्ट प्रति रूपमा, 2018...\n",
              "1   Delhi techie wins free food from Swiggy for on...  ...  Kunal शाह क्रेडिट कार्ड बिल भुक्तानी मंच, Cred...\n",
              "60  Govt to present Interim Budget, not full Budge...  ...  बुधवार मा वित्त मन्त्रालयले स्पष्टीकरण पूर्ण ब...\n",
              "23  Isha Ambani features on February cover of Vogu...  ...  रिलायन्स इंडस्ट्रीज 'अध्यक्ष मुकेश अंबानी छोरी...\n",
              "65      Qatar to invest $200 million in Airtel Africa  ...  बुधबार एयरटेल कतार गरेको सार्वभौम धन कोष कतार ...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQbUe7JJUmGQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6703a86c-ea0f-449b-b693-d07bf12dc712"
      },
      "source": [
        "\n",
        "document,summary= pd.DataFrame(), pd.DataFrame()\n",
        "document['CTEXT'] = news['CTEXT']\n",
        "summary['HEADLINES'] = news['HEADLINES']\n",
        "\n",
        "document.shape,summary.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100, 1), (100, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx9uyY33g2nW"
      },
      "source": [
        "# replacing many abbreviations and lower casing the words\r\n",
        "def clean_words(sentence):\r\n",
        "    sentence = str(sentence).lower()\r\n",
        "    # sentence = unicodedata.normalize('NFKD', sentence).encode('ascii', 'ignore').decode('utf-8', 'ignore') # for converting é to e and other accented chars\r\n",
        "    sentence = re.sub(r\"\\\"\", \"\", sentence)\r\n",
        "    sentence = re.sub(r\"\\'\", \"\", sentence)\r\n",
        "    sentence = re.sub(r\"[\\[\\]\\\\()\\\"$#%/@;:<>{}`+=~|.!?,-]\", \"\", sentence)\r\n",
        "    sentence = re.sub(r\"&\", \"\", sentence)\r\n",
        "    sentence = re.sub(r\"\\\\n\", \"\", sentence)\r\n",
        "    sentence = sentence.strip()\r\n",
        "    return sentence "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qToCbO9nhX62"
      },
      "source": [
        "summary['HEADLINES'] = summary['HEADLINES'].apply(lambda x: clean_words(x))\r\n",
        "document['CTEXT'] = document['CTEXT'].apply(lambda x: clean_words(x))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8gKyq1gIq4r"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ6LE4MrJjC_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "34b9f911-2d3c-46b2-eda9-aab3374f8b26"
      },
      "source": [
        "# for decoder sequence\n",
        "start_token, end_token = '<startseq>' , '<endseq>'\n",
        "summary = summary.apply(lambda x: start_token + ' ' + x + ' ' + end_token)\n",
        "summary.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HEADLINES</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>&lt;startseq&gt; oneplus 2018 को प्रिमियम स्मार्टफोन...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;startseq&gt; दिल्ली प्रौद्योगिकी विशेषज्ञ cred ए...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>&lt;startseq&gt; फेब्रुअरी 1 मा उपस्थित अन्तरिम बजेट...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>&lt;startseq&gt; ईशा अंबानी वोग पत्रिका को फेब्रुअरी...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>&lt;startseq&gt; एयरटेल अफ्रीका लगानी गर्न कतार लाख ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            HEADLINES\n",
              "53  <startseq> oneplus 2018 को प्रिमियम स्मार्टफोन...\n",
              "1   <startseq> दिल्ली प्रौद्योगिकी विशेषज्ञ cred ए...\n",
              "60  <startseq> फेब्रुअरी 1 मा उपस्थित अन्तरिम बजेट...\n",
              "23  <startseq> ईशा अंबानी वोग पत्रिका को फेब्रुअरी...\n",
              "65  <startseq> एयरटेल अफ्रीका लगानी गर्न कतार लाख ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhiChcwHWppE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f5f22d-3d40-475c-a262-b8cb7afee398"
      },
      "source": [
        "val_split=0.1\n",
        "#train validation split\n",
        "summary_train = summary[int(len(summary)*val_split):]\n",
        "summary_val = summary[:int(len(summary)*val_split)]\n",
        "document_train = document[int(len(summary)*val_split):]\n",
        "document_val = document[:int(len(summary)*val_split)]\n",
        "\n",
        "len(document_val),len(document_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 90)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTTmp4RnXGiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb8730d8-5ff8-47f2-cd35-452eb65632ec"
      },
      "source": [
        "document_train.iloc[0], summary_train.iloc[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(CTEXT    aegon जीवन iterm बीमा योजना संग ग्राहकहरु आफ्न...\n",
              " Name: 3, dtype: object,\n",
              " HEADLINES    <startseq> aegon जीवन iterm बीमा योजना कर सेव ...\n",
              " Name: 3, dtype: object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LinLiK7JfltG"
      },
      "source": [
        "FINDING MAX LENGTH OF TEXT AND SUMMARY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFgJ9DTjfjhh",
        "outputId": "aec858da-76b1-44c8-f868-a6ea61dfab47"
      },
      "source": [
        "\r\n",
        "# because there are senteces with unusually long lengths, \r\n",
        "# we caculate the max length that 95% of sentences are shorter than that\r\n",
        "def max_length(shorts, longs, prct):\r\n",
        "    # Create a list of all the captions\r\n",
        "    \r\n",
        "    length_longs = list(len(d.split()) for d in longs)\r\n",
        "    length_shorts = list(len(d.split()) for d in shorts)\r\n",
        "\r\n",
        "    print('percentile {} of length of news: {}'.format(prct,np.percentile(length_longs, prct)))\r\n",
        "    print('longest sentence: ', max(length_longs))\r\n",
        "    print()\r\n",
        "    print('percentile {} of length of summaries: {}'.format(prct,np.percentile(length_shorts, prct)))\r\n",
        "    print('longest sentence: ', max(length_shorts))\r\n",
        "    print()\r\n",
        "    return int(np.percentile(length_longs, prct)),int(np.percentile(length_shorts, prct))\r\n",
        "\r\n",
        "# selecting sentence length based on the percentile of data that fits in the length\r\n",
        "max_len_news, max_len_summary= max_length(summary_train['HEADLINES'].to_list(), document_train['CTEXT'].to_list(), 90)\r\n",
        "\r\n",
        "\r\n",
        "print('max-length document chosen for training: ', max_len_news)\r\n",
        "print('max-length summaries chosen for training: ', max_len_summary)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "percentile 90 of length of news: 60.0\n",
            "longest sentence:  69\n",
            "\n",
            "percentile 90 of length of summaries: 14.0\n",
            "longest sentence:  17\n",
            "\n",
            "max-length document chosen for training:  60\n",
            "max-length summaries chosen for training:  14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0jeoHo2fvol"
      },
      "source": [
        "Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yln3p8z-f1gX"
      },
      "source": [
        "def create_vocab(shorts, longs = None, minimum_repeat = 3):\r\n",
        "\r\n",
        "    # Create a list of all the captions\r\n",
        "    all_captions = []\r\n",
        "    for s in shorts:\r\n",
        "        all_captions.append(s)\r\n",
        "\r\n",
        "    # Consider only words which occur at least minimum_occurrence times in the corpus\r\n",
        "    word_counts = {}\r\n",
        "    nsents = 0\r\n",
        "    for sent in all_captions:\r\n",
        "        nsents += 1\r\n",
        "        for w in sent.split(' '):\r\n",
        "            word_counts[w] = word_counts.get(w, 0) + 1\r\n",
        "\r\n",
        "    vocab = [w for w in word_counts if word_counts[w] >= minimum_repeat]\r\n",
        "    \r\n",
        "    vocab = list(set(vocab))\r\n",
        "    return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1Huj8Z4f_T3",
        "outputId": "156a7e31-751e-41c7-f63c-35bf8fca049e"
      },
      "source": [
        "\r\n",
        "# each word in the vocabulary must be used in the data atleast minimum_repeat times\r\n",
        "vocab_dec = create_vocab(summary_train['HEADLINES'].to_list(), minimum_repeat=3) # here we just use the words in vocabulary of summaries\r\n",
        "# removing one character words from vocab except for 'a'\r\n",
        "# for v in vocab_dec:\r\n",
        "#     if len(v) == 1 and v!='अ' and v!='क':\r\n",
        "#         vocab_dec.remove(v) \r\n",
        "        \r\n",
        "vocab_dec = sorted(vocab_dec)[1:] # [1:] is for the '' \r\n",
        "vocab_dec[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<endseq>',\n",
              " '<startseq>',\n",
              " 'pm',\n",
              " 'अभिनेत्री',\n",
              " 'अमेरिकी',\n",
              " 'आफ्नो',\n",
              " 'आरोप',\n",
              " 'एक',\n",
              " 'काम',\n",
              " 'को']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kdu_HmHYgFmI",
        "outputId": "e14bae69-8a9b-4b70-c8f4-b42d21d8b468"
      },
      "source": [
        "\r\n",
        "# each word in the vocabulary must be used in the data atleast minimum_repeat times\r\n",
        "vocab_enc = create_vocab(document_train['CTEXT'].to_list(), minimum_repeat=3) # here we just use the words in vocabulary of summaries\r\n",
        "# removing one character words from vocab except for 'a'\r\n",
        "# for v in vocab_enc:\r\n",
        "#     if len(v) == 1 and v!='a' and v!='i':\r\n",
        "#         vocab_enc.remove(v) \r\n",
        "        \r\n",
        "vocab_enc = sorted(vocab_enc)[1:] # [1:] is for the '' \r\n",
        "vocab_enc[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['10',\n",
              " '12',\n",
              " '2018',\n",
              " '22',\n",
              " '90',\n",
              " 'brawl',\n",
              " 'commode',\n",
              " 'eufs',\n",
              " 'filmmaker',\n",
              " 'hirani']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd9VGM5fgMDf",
        "outputId": "5ea20095-2248-4e6c-bd89-75fb61f9b295"
      },
      "source": [
        "oov_token = '<UNK>'\r\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n' # making sure all the last non digit non alphabet chars are removed\r\n",
        "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = filters,oov_token=oov_token)\r\n",
        "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = filters,oov_token=oov_token)\r\n",
        "document_tokenizer.fit_on_texts(vocab_enc)\r\n",
        "summary_tokenizer.fit_on_texts(vocab_dec)#summaries_train['short'])\r\n",
        "\r\n",
        "# caculating number of words in vocabulary of encoder and decoder\r\n",
        "# they are important for positional encoding\r\n",
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1 \r\n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\r\n",
        "\r\n",
        "# vocab_size\r\n",
        "encoder_vocab_size, decoder_vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(357, 40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osX0P7fSgT5Q"
      },
      "source": [
        "ixtoword_enc = {} # index to word dic\r\n",
        "ixtoword_dec = {} # index to word dic\r\n",
        "\r\n",
        "wordtoix_enc = document_tokenizer.word_index # word to index dic\r\n",
        "ixtoword_enc[0] = '<PAD0>' # no word in vocab has index 0. but padding is indicated with 0\r\n",
        "ixtoword_dec[0] = '<PAD0>' # no word in vocab has index 0. but padding is indicated with 0\r\n",
        "\r\n",
        "for w in document_tokenizer.word_index:\r\n",
        "    ixtoword_enc[document_tokenizer.word_index[w]] = w\r\n",
        "################################################\r\n",
        "wordtoix_dec = summary_tokenizer.word_index # word to index dic\r\n",
        "\r\n",
        "for w in summary_tokenizer.word_index:\r\n",
        "    ixtoword_dec[summary_tokenizer.word_index[w]] = w\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Zv7FIvKbTi"
      },
      "source": [
        "#### Tokenizing the texts into integer tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TqbpEyPMRqa"
      },
      "source": [
        "# # since < and > from default tokens cannot be removed\n",
        "# filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "# oov_token = '<unk>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHw2csoYImsa"
      },
      "source": [
        "# document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
        "# summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWU9Xu7OKVab"
      },
      "source": [
        "\n",
        "# document_tokenizer.fit_on_texts(document)\n",
        "# summary_tokenizer.fit_on_texts(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ESm-aYR-tvx"
      },
      "source": [
        "inputs = document_tokenizer.texts_to_sequences(document_train['CTEXT'])\n",
        "targets = summary_tokenizer.texts_to_sequences(summary_train['HEADLINES'])\n",
        "\n",
        "inputs_val= document_tokenizer.texts_to_sequences(document_val['CTEXT'])\n",
        "targets_val= document_tokenizer.texts_to_sequences(summary_val['HEADLINES'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVyErXAei5_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c33430-a2df-4bd1-ef78-ba12f139a11b"
      },
      "source": [
        "summary_tokenizer.texts_to_sequences([\"‘लभ स्टेसन’ भियतनाम र इन्डोनेसियामा\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh0wwyqusZN9"
      },
      "source": [
        "#yaha bata tala ko bert tokenizer use gardeko ho try garna ko lagi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIGVVCCOMzib"
      },
      "source": [
        "# # since * and ~ from default tokens cannot be removed\r\n",
        "# filters = '!\"#$%&()+,-./:;=?@[\\\\]^_`{|}\\t\\n'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCyp7EB-oHii"
      },
      "source": [
        "# !pip install transformers\n",
        "# from transformers import BertTokenizer\n",
        "\n",
        "# # load bert tokenizer\n",
        "# print(\"loading BERT  tokenizer...\")\n",
        "# document_tokenizer=BertTokenizer.from_pretrained('bert-base-multilingual-cased',do_lower=True)\n",
        "# summary_tokenizer=BertTokenizer.from_pretrained('bert-base-multilingual-cased',do_lower=True, filters=filters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eGVNX4ir4cp"
      },
      "source": [
        "# sent=(\"सहज रुपमा भन्नुपर्दा नेपाली भनेको नेपालमा बस्ने नागरिक हो । हामी नेपाली नेपाललाई अंग्रेजबाट मुक्त गराएर स्वच्छ बनाउने बिर योद्धा हो ।\")\n",
        "# decoder_input=(summary_tokenizer.vocab_size)\n",
        "# print(decoder_input)\n",
        "\n",
        "# encodded=summary_tokenizer.encode(sent, add_special_tokens= True)\n",
        "# print(\"encodded yesko tala\")\n",
        "# print(encodded)\n",
        "\n",
        "# encoder_input = tf.expand_dims(encodded, 0)\n",
        "# print(encoder_input)\n",
        "\n",
        "# token=summary_tokenizer.tokenize(sent)\n",
        "# print(\"\\n yesko tala tokens\")\n",
        "# print(token)\n",
        "\n",
        "# ids=summary_tokenizer.convert_tokens_to_ids(token)\n",
        "# print(\"\\n yesko tala ids\")\n",
        "# print(ids)\n",
        "\n",
        "# tokens=summary_tokenizer.convert_ids_to_tokens(ids)\n",
        "# print(\"\\n yesko tala tokens\")\n",
        "# print(tokens)\n",
        "\n",
        "# print(\"\\n yesko tala string\")\n",
        "# print(summary_tokenizer.convert_tokens_to_string(tokens))\n",
        "\n",
        "\n",
        "# print(summary_tokenizer.tokenize(\"CLS\"))\n",
        "# print(summary_tokenizer.convert_tokens_to_ids(\n",
        "# summary_tokenizer.tokenize(\"~\")))\n",
        "# print(summary_tokenizer.convert_tokens_to_ids(\n",
        "# summary_tokenizer.tokenize(\"*\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoPdFplmta4F"
      },
      "source": [
        "# val_split=0.1\r\n",
        "# #train validation split\r\n",
        "# summary_train = summary[int(len(summary)*val_split):]\r\n",
        "# summary_val = summary[:int(len(summary)*val_split)]\r\n",
        "# document_train = document[int(len(document)*val_split):]\r\n",
        "\r\n",
        "# document_val = document[:int(len(document)*val_split)]\r\n",
        "\r\n",
        "# len(document_val),len(document_train),len(summary_val),len(summary_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDB2SLgm-j2b"
      },
      "source": [
        "\r\n",
        "# document_val.reset_index(inplace=True, drop=True)\r\n",
        "# summary_val.reset_index(inplace=True, drop=True)\r\n",
        "# document_train.reset_index(inplace=True, drop=True)\r\n",
        "# summary_train.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAGmI6U0sjuX"
      },
      "source": [
        "# # tokenize all of the  input sentences and map the tokens to their respective word ids\n",
        "# inputs=[]\n",
        "\n",
        "# #for every sentences\n",
        "# for sent in document_train:\n",
        "#   encodded_sent=document_tokenizer.encode(\n",
        "#                     sent, add_special_tokens=True,\n",
        "#                         )\n",
        "#   inputs.append(encodded_sent)\n",
        "\n",
        "# # print first paragraph now as list of ids.\n",
        "# print('original:',document_train[0])\n",
        "# print('token IDs',inputs[0])\n",
        "\n",
        "# # tokenize all of the target sentences and map the tokens to their respective word ids\n",
        "# targets=[]\n",
        "\n",
        "# #for every sentences\n",
        "# for sent in summary_train:\n",
        "#   encodded_sent=summary_tokenizer.encode(\n",
        "#                     sent, add_special_tokens=True,\n",
        "#                         )\n",
        "#   targets.append(encodded_sent)\n",
        "\n",
        "# # print first paragraph now as list of ids.\n",
        "# print('original:',summary_train[0])\n",
        "# print('token IDs',targets[0])\n",
        "\n",
        "# print(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCcgMR5FtNmp"
      },
      "source": [
        "# # tokenize all of the  input sentences and map the tokens to their respective word ids\r\n",
        "# inputs_val=[]\r\n",
        "\r\n",
        "# #for every sentences\r\n",
        "# for sent in document_val:\r\n",
        "#   encodded_sent=document_tokenizer.encode(\r\n",
        "#                     sent, add_special_tokens=True,\r\n",
        "#                         )\r\n",
        "#   inputs_val.append(encodded_sent)\r\n",
        "\r\n",
        "# # print first paragraph now as list of ids.\r\n",
        "# print('original:',document_val[0])\r\n",
        "# print('token IDs',inputs_val[0])\r\n",
        "\r\n",
        "# # tokenize all of the target sentences and map the tokens to their respective word ids\r\n",
        "# targets_val=[]\r\n",
        "\r\n",
        "# #for every sentences\r\n",
        "# for sent in summary_val:\r\n",
        "#   encodded_sent=summary_tokenizer.encode(\r\n",
        "#                     sent, add_special_tokens=True,\r\n",
        "#                         )\r\n",
        "#   targets_val.append(encodded_sent)\r\n",
        "\r\n",
        "# # print first paragraph now as list of ids.\r\n",
        "# print('original:',summary_val[0])\r\n",
        "# print('token IDs',targets_val[0])\r\n",
        "\r\n",
        "# print(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9iYqCvruQnr"
      },
      "source": [
        "# encoder_vocab_size = len(inputs[]) +1\n",
        "# decoder_vocab_size = len(targets[0]) +1\n",
        "\n",
        "# # vocab_size\n",
        "# encoder_vocab_size, decoder_vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phq1_BXO0b4R"
      },
      "source": [
        "# # get encoder and decoder vocab size\n",
        "# enc_greatest=[]\n",
        "# for inp in inputs:\n",
        "#   enc_greatest.append(max(inp))\n",
        "# encoder_vocab_size=(max(enc_greatest))  +1\n",
        "\n",
        "\n",
        "\n",
        "# # get encoder and decoder vocab size\n",
        "# greatest=[]\n",
        "# for inp in targets:\n",
        "#   greatest.append(max(inp))\n",
        "# decoder_vocab_size=max(greatest) + 1   \n",
        "\n",
        "\n",
        "# # encoder_vocab_size=119548\n",
        "# # decoder_vocab_size=119548\n",
        "\n",
        "# encoder_vocab_size, decoder_vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdSnIfXRnfw-"
      },
      "source": [
        "# summary_tokenizer.convert_ids_to_tokens([19,43,432,44,41])\r\n",
        "# summary_tokenizer.convert_tokens_to_string(summary_tokenizer.convert_ids_to_tokens([13,43,432,4412,41]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqtq3__ppzqS"
      },
      "source": [
        "# inputs = document_tokenizer.encode(document_train)\r\n",
        "# targets = summary_tokenizer.encode(summary_train)\r\n",
        "\r\n",
        "# inputs_val= document_tokenizer.encode(document_val)\r\n",
        "# targets_val= document_tokenizer.encode(summary_val)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBz87vw1wO0p"
      },
      "source": [
        "#yaha bhanda mathi ho bert tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryx9qx90jwXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e50e8e0c-54dc-4d1d-e536-9c9b1ced2971"
      },
      "source": [
        "summary_tokenizer.sequences_to_texts([[13,43,432,4412,41]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['छ <UNK> <UNK> <UNK> <UNK>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoizyBvLKv8h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33fb3967-d26f-4399-b321-a057a57b4cb8"
      },
      "source": [
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(357, 40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZden_q9_eZr"
      },
      "source": [
        "#### Obtaining insights on lengths for defining maxlen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma4o2nGdK5Xb"
      },
      "source": [
        "document_lengths = pd.Series([len(x) for x in document])\n",
        "summary_lengths = pd.Series([len(x) for x in summary])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXZlO99C-UXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39e18ff-0081-4b78-9318-7b3c11e8d305"
      },
      "source": [
        "document_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1.0\n",
              "mean     5.0\n",
              "std      NaN\n",
              "min      5.0\n",
              "25%      5.0\n",
              "50%      5.0\n",
              "75%      5.0\n",
              "max      5.0\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALMwKMx--ZF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46175a45-34a9-4fa5-d9e2-07b7ba8d8506"
      },
      "source": [
        "summary_lengths.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1.0\n",
              "mean     9.0\n",
              "std      NaN\n",
              "min      9.0\n",
              "25%      9.0\n",
              "50%      9.0\n",
              "75%      9.0\n",
              "max      9.0\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVeMilXr-bpC"
      },
      "source": [
        "# # maxlen\n",
        "# # taking values > and round figured to 75th percentile\n",
        "# # at the same time not leaving high variance/\n",
        "# max_len_news=encoder_vocab_size+1\n",
        "# max_len_summary=int(encoder_vocab_size/3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SWap3YJBk-D"
      },
      "source": [
        "#### Padding/Truncating sequences for identical sequence lengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEyUBeu7ACRt"
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_len_news, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=max_len_summary, padding='post', truncating='post')\n",
        "\n",
        "inputs_val = tf.keras.preprocessing.sequence.pad_sequences(inputs_val, maxlen=max_len_news, padding='post', truncating='post')\n",
        "targets_val = tf.keras.preprocessing.sequence.pad_sequences(targets_val, maxlen=max_len_summary, padding='post', truncating='post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIP0kIIcB8Rm"
      },
      "source": [
        "### Creating dataset pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzO6l3-AB7hJ"
      },
      "source": [
        "# inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "# targets = tf.cast(targets, dtype=tf.int32)\n",
        "\n",
        "# inputs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI-fV7eABWN6"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((inputs_val, targets_val)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE*2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-wXos1E_p1d"
      },
      "source": [
        "document_val.reset_index(inplace=True, drop=True)\r\n",
        "summary_val.reset_index(inplace=True, drop=True)\r\n",
        "document_train.reset_index(inplace=True, drop=True)\r\n",
        "summary_train.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wF-XzFKadlq"
      },
      "source": [
        "# defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3_Cs5-0ajIZ"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def hist(history):\n",
        "#     plt.title('Loss')\n",
        "\n",
        "#     x= [i[0] for i in history['val']]\n",
        "#     y=[i[1] for i in history['val']]\n",
        "#     plt.plot(x,y,'x-')\n",
        "    \n",
        "#     x= [i[0] for i in history['train']]\n",
        "#     y=[i[1] for i in history['train']]    \n",
        "#     plt.plot(x,y,'o-')\n",
        "\n",
        "#     plt.legend(['validation','train'])\n",
        "#     plt.show()\n",
        "#     print('smallest val loss:', sorted(history['val'],key=lambda x: x[1])[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isN1CpAXLfsl"
      },
      "source": [
        "### Positional Encoding for adding notion of position among words as unlike RNN this is non-directional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Purv7oyhETDZ"
      },
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40J2pc2NEXp5"
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Pe01DMMWHc"
      },
      "source": [
        "### Masking\n",
        "\n",
        "- Padding mask for masking \"pad\" sequences\n",
        "- Lookahead mask for masking future words from contributing in prediction of current words in self attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN1wVQAdMVYy"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmjAPLWuMREE"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8DqUBc4NFOy"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfknVF7hNKf7"
      },
      "source": [
        "#### Scaled Dot Product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_B6M9OBNBKB"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf7_a5uQOfJk"
      },
      "source": [
        "#### Multi-Headed Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIuFrdXnNZEC"
      },
      "source": [
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        # self.wq = tf.keras.layers.Dense(d_model)\n",
        "        # self.wk = tf.keras.layers.Dense(d_model)\n",
        "        # self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        # self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model,kernel_regularizer=tf.keras.regularizers.l2(l=lmbda_l2))\n",
        "        self.wk = tf.keras.layers.Dense(d_model,kernel_regularizer=tf.keras.regularizers.l2(l=lmbda_l2))\n",
        "        self.wv = tf.keras.layers.Dense(d_model,kernel_regularizer=tf.keras.regularizers.l2(l=lmbda_l2))\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model,kernel_regularizer=tf.keras.regularizers.l2(l=lmbda_l2))\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "            \n",
        "        return output, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYPcSVj1bdyb"
      },
      "source": [
        "# embeddings preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUio-t4RocrB",
        "outputId": "49e1d417-ccc1-43ae-ccba-e0a1eb8bb923"
      },
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-20 09:03:51--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-02-20 09:03:52--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-02-20 09:03:52--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip         10%[=>                  ]  90.01M  2.00MB/s    eta 6m 7s  ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW88qeN6ofYn",
        "outputId": "66cc8948-d0a2-4a8c-adea-0eb3424f2e87"
      },
      "source": [
        "# !unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of glove.6B.zip or\n",
            "        glove.6B.zip.zip, and cannot find glove.6B.zip.ZIP, period.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t238OgdaojgT",
        "outputId": "5b885217-b477-4339-8d3e-b667f7715b81"
      },
      "source": [
        "# !ls\r\n",
        "# !pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  glove.6B.zip  sample_data\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg2Tb9QC_Ttd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83c581c0-988f-430c-8c6b-fe318967cecd"
      },
      "source": [
        "# Making the embedding mtrix\r\n",
        "def make_embedding_layer(vocab_len, wordtoix, embedding_dim=300, glove=True, glove_path= '../glove'):\r\n",
        "    if glove == False:\r\n",
        "        print('Just a zero matrix loaded')\r\n",
        "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # just a zero matrix \r\n",
        "    else:\r\n",
        "        print('Loading glove...')\r\n",
        "        glove_dir = \"/content/drive/MyDrive/Datasets/\"\r\n",
        "        embeddings_index = {} \r\n",
        "        fi = open(glove_dir +'nepali_embeddings_word2vec.txt', encoding=\"utf-8\")\r\n",
        "    \r\n",
        "        for line in fi:\r\n",
        "            values = line.split()\r\n",
        "            word = values[0]\r\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\r\n",
        "            embeddings_index[word] = coefs\r\n",
        "        fi.close()\r\n",
        "        # Get n-dim dense vector for each of the vocab_rocc\r\n",
        "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # to import as weights for Keras Embedding layer\r\n",
        "        for word, i in wordtoix.items():\r\n",
        "            embedding_vector = embeddings_index.get(word)\r\n",
        "            if embedding_vector is not None:\r\n",
        "                # Words not found in the embedding index will be all zeros\r\n",
        "                embedding_matrix[i] = embedding_vector\r\n",
        "        \r\n",
        "        print(\"GloVe \",embedding_dim, ' loaded!')\r\n",
        "\r\n",
        "    embedding_layer = Embedding(vocab_len, embedding_dim, mask_zero=True, trainable=False) # we have a limited vocab so we \r\n",
        "                                                                                           # do not train the embedding layer\r\n",
        "                                                                                           # we use 0 as padding so => mask_zero=True\r\n",
        "    embedding_layer.build((None,))\r\n",
        "    embedding_layer.set_weights([embedding_matrix])\r\n",
        "    return embedding_layer\r\n",
        "\r\n",
        "embeddings_encoder = make_embedding_layer(encoder_vocab_size, wordtoix_enc, embedding_dim=embedding_dim, glove=True)\r\n",
        "embeddings_decoder = make_embedding_layer(decoder_vocab_size, wordtoix_dec, embedding_dim=embedding_dim, glove=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading glove...\n",
            "GloVe  300  loaded!\n",
            "Loading glove...\n",
            "GloVe  300  loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A49tXMVvOkOZ"
      },
      "source": [
        "### Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zi03-x3cXdx"
      },
      "source": [
        "# hyper-params\n",
        "init_lr = 1e-3\n",
        "lmbda_l2 = 0.1\n",
        "d_out_rate = 0.1\n",
        "num_layers = 4\n",
        "d_model = embedding_dim # d_model is the representation dimension or embedding dimension of a word (usually in the range 128–512)\n",
        "dff = 512 # number of neurons in feed forward network\n",
        "num_heads = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9-qoKuTNwKq"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2RRmn2bOpW9"
      },
      "source": [
        "#### Fundamental Unit of Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNuoJoFWO335"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i6Zh8gnPqdW"
      },
      "source": [
        "#### Fundamental Unit of Transformer decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVmvs6dPMRC"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zt5MUc_QNid"
      },
      "source": [
        "#### Encoder consisting of multiple EncoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrbnTwijQJ-h"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N5LrNrvRexg"
      },
      "source": [
        "#### Decoder consisting of multiple DecoderLayer(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmeqkZrIRbSB"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbMNK_bzSHnh"
      },
      "source": [
        "#### Finally, the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXHRG-o4R9Mc"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.15):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UndsMPZXTdSr"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMTZJdIoSbuy"
      },
      "source": [
        "\n",
        "\n",
        "# EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOGvkYDNTjIj"
      },
      "source": [
        "#### Adam optimizer with custom learning rate scheduling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfiynCLlTL8C"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsVdrENTUERY"
      },
      "source": [
        "#### Defining losses and other metrics "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip1-943kTXXK"
      },
      "source": [
        "# learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuYuXiRfdWqb"
      },
      "source": [
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=init_lr, \n",
        "    decay_steps=4000, # approximately 5 epochs\n",
        "    decay_rate=0.95) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCItdIOSfRhU"
      },
      "source": [
        "\n",
        "optimizer2 = Adam(lr_schedule , beta_1=0.9, beta_2=0.98, epsilon=1e-9) # changed to init\n",
        "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none') # added softmax changed from_logits to false"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW4LA_45T4Aa"
      },
      "source": [
        "def loss_function(real, pred, l2= False):\n",
        " \n",
        "    if l2:\n",
        "        lambda_ = 0.0001\n",
        "        l2_norms = [tf.nn.l2_loss(v) for v in transformer.trainable_variables]\n",
        "        l2_norm = tf.reduce_sum(l2_norms)\n",
        "        l2_value = lambda_ * l2_norm\n",
        "        loss_ = loss_object(real, pred) + l2_value\n",
        "    else:\n",
        "        loss_ = loss_object(real, pred) \n",
        "    \n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze0u6xxXT7dI"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XvKy3v6ULnO"
      },
      "source": [
        "#### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5-RcxqFUCuk"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    encoder_vocab_size, \n",
        "    decoder_vocab_size, \n",
        "    pe_input=encoder_vocab_size, \n",
        "    pe_target=decoder_vocab_size,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f56BGiVXU_Dk"
      },
      "source": [
        "#### Masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZxHuyZxU5Pa"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYIotvaBVI0d"
      },
      "source": [
        "#### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOc1_3c-VGaL"
      },
      "source": [
        "checkpoint_path4 =\"/content/drive/MyDrive/checkpoints5\"\n",
        "\n",
        "ckpt4 = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer2)\n",
        "\n",
        "ckpt_manager4 = tf.train.CheckpointManager(ckpt4, checkpoint_path4, max_to_keep=100)\n",
        "\n",
        "if ckpt_manager4.latest_checkpoint:\n",
        "    ckpt4.restore(ckpt_manager4.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfpI0gS4c06c"
      },
      "source": [
        "#### Training steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmVOMzkrczgl"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer2.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xORKpv69dSW5"
      },
      "source": [
        "# for epoch in range(EPOCHS):\n",
        "#     start = time.time()\n",
        "\n",
        "#     train_loss.reset_states()\n",
        "  \n",
        "#     for (batch, (inp, tar)) in enumerate(dataset):\n",
        "#         train_step(inp, tar)\n",
        "    \n",
        "#         # 55k samples\n",
        "#         # we display 3 batch results -- 0th, middle and last one (approx)\n",
        "#         # 55k / 64 ~ 858; 858 / 2 = 429\n",
        "#         if batch % 429 == 0:\n",
        "#             print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "      \n",
        "#     if (epoch + 1) % 5 == 0:\n",
        "#         ckpt_save_path = ckpt_manager.save()\n",
        "#         print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "\n",
        "#     # save best model til now\n",
        "\n",
        "    \n",
        "#     print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "\n",
        "#     print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVbEUCZagJ0G"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMbqGTixu1cl"
      },
      "source": [
        "#### Predicting one word at a time at the decoder and appending it to the output; then taking the complete sequence as an input to the decoder and repeating until maxlen or stop keyword appears"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5D5cv2Jd8-6"
      },
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=max_len_news, \n",
        "                                                                           padding='post', truncating='post')\n",
        "    \n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[start_token]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(max_len_summary):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "        # stop prediciting if it reached end_token\n",
        "        if predicted_id == summary_tokenizer.word_index[end_token]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # remove start_token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKq2IqXIr3p6"
      },
      "source": [
        "#yo bert tokenizer ko lagi xuttai Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnvBGUTnKZ23"
      },
      "source": [
        "\n",
        "# def evaluate(input_document):\n",
        "#     input_document = document_tokenizer.tokenize(input_document)\n",
        "#     input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "#     encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "#     decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n",
        "#     output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "#     for i in range(decoder_maxlen):\n",
        "#         enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "#         predictions, attention_weights = transformer(\n",
        "#             encoder_input, \n",
        "#             output,\n",
        "#             False,\n",
        "#             enc_padding_mask,\n",
        "#             combined_mask,\n",
        "#             dec_padding_mask\n",
        "#         )\n",
        "\n",
        "#         predictions = predictions[: ,-1:, :]\n",
        "#         predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "#         if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
        "#             return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "#         output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "#     return tf.squeeze(output, axis=0), attention_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIDuBR2bYNVX"
      },
      "source": [
        "mathi ko le kaam garena tei vayera naya halera try gareko tala ko le"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wazDwAZxYULL"
      },
      "source": [
        "\n",
        "# def evaluate(inp_sentence):\n",
        "#   # start_token = [document_tokenizer.vocab_size]\n",
        "#   # end_token = [document_tokenizer.vocab_size + 1]\n",
        "\n",
        "#   # inp sentence is portuguese, hence adding the start and end token\n",
        "#   print(inp_sentence)\n",
        "#   inp_sentence =  document_tokenizer.encode(inp_sentence)\n",
        "#   print(inp_sentence)\n",
        "#   encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "#   # encoder_input=encoder_vocab_size\n",
        "\n",
        "#   # as the target is english, the first word to the transformer should be the\n",
        "#   # english start token.\n",
        "#   decoder_input = [decoder_vocab_size-1]\n",
        "#   output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "\n",
        "#   for i in range(40):\n",
        "#     enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "#         encoder_input, output)\n",
        "\n",
        "#     # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "#     predictions, attention_weights = transformer(encoder_input, \n",
        "#                                                  output,\n",
        "#                                                  False,\n",
        "#                                                  enc_padding_mask,\n",
        "#                                                  combined_mask,\n",
        "#                                                  dec_padding_mask)\n",
        "\n",
        "#     # # select the last word from the seq_len dimension\n",
        "#     # predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "#     # predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "#     # # return the result if the predicted_id is equal to the end token\n",
        "#     # if predicted_id == decoder_vocab_size+1:\n",
        "#     #   return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "#     # concatentate the predicted_id to the output which is given to the decoder\n",
        "#     # as its input.\n",
        "#     # output = tf.concat([output, predicted_id], axis=-1)\n",
        "#     print(\"\\n\")\n",
        "#     print(predictions)\n",
        "#     output=predictions\n",
        "#     print(f\" output is {output}\")\n",
        "\n",
        "  \n",
        "\n",
        "#   return  output \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E88xoNLaZ_k"
      },
      "source": [
        "# def summarize(sentence, plot=''):\n",
        "#   result= evaluate(sentence)\n",
        "\n",
        "#   predicted_sentence = summary_tokenizer.decode([i for i in result \n",
        "#                                             if i < summary_tokenizer.vocab_size])  \n",
        "\n",
        "#   print('Input: {}'.format(sentence))\n",
        "#   print('Predicted translation: {}'.format(predicted_sentence))\n",
        "\n",
        "#   if plot:\n",
        "#     plot_attention_weights(attention_weights, sentence, result, plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14gKTWIqsaFk"
      },
      "source": [
        "# def summarize(input_document):\n",
        "#     # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "#     summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "#     # summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
        "#     return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcJyyT3vsbH1"
      },
      "source": [
        "#bert evaluate yaha bhanda mathi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkpdiW6wnmiS"
      },
      "source": [
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9i9HW6yj4lN"
      },
      "source": [
        "def validate():\n",
        "    print('validation started ...')\n",
        "    val_loss.reset_states()\n",
        "    for (batch, (inp, tar)) in enumerate(dataset_val):    \n",
        "        tar_inp = tar[:, :-1] # <startseq> hi im moein\n",
        "        tar_real = tar[:, 1:] # hi im moein <endseq>\n",
        "\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "        # Operations are recorded if they are executed within this context manager\n",
        "        # and at least one of their inputs is being \"watched\". Trainable variables are automatically watched\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            False, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        val_loss(loss)\n",
        "    print('\\n* Validation loss: {} '.format(val_loss.result()) )\n",
        "    return val_loss.result()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsEL0dglkESj"
      },
      "source": [
        "history={'val':[],'train':[]}\n",
        "EPOCHS = 30\n",
        "not_progressing = 0\n",
        "# Computes the (weighted) mean of the given loss values.\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "val_loss = tf.keras.metrics.Mean(name='val_loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKXhZd_BkJjL",
        "outputId": "993d0038-ce96-4b14-8487-28bfefaea78c"
      },
      "source": [
        "\n",
        "params = {\n",
        "'lmbda_l2' : lmbda_l2,\n",
        "'d_out_rate' :d_out_rate,\n",
        "'num_layers' : num_layers ,\n",
        "'d_model' : d_model  ,\n",
        "'dff' : dff ,\n",
        "'num_heads' : num_heads,\n",
        "'init_lr':init_lr}\n",
        "params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'d_model': 300,\n",
              " 'd_out_rate': 0.1,\n",
              " 'dff': 512,\n",
              " 'init_lr': 0.001,\n",
              " 'lmbda_l2': 0.1,\n",
              " 'num_heads': 5,\n",
              " 'num_layers': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 500
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10s_w1JDkOD0"
      },
      "source": [
        "ep = 1\n",
        "best_val_loss = np.inf\n",
        "i1,i2,i3,i4 = np.random.randint(len(summary_val)),np.random.randint(len(summary_val)),np.random.randint(len(summary_val)),np.random.randint(len(summary_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H7xtCQJkajw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "47f1f27d-a9c5-491a-b55b-37b54a82b4fe"
      },
      "source": [
        "print(params)\n",
        "print('#'*40)\n",
        "\n",
        "for epoch in range(ep,EPOCHS+1):\n",
        "    ep = epoch\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        \n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        if batch % 150 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch , batch, train_loss.result()))\n",
        "                  \n",
        "    print()\n",
        "    #print(summarize(document_val[i1]))\n",
        "    # print(summarize(document_val[i2]))\n",
        "    # print(summarize(document_val[i3]))\n",
        "    # print(summarize(document_val[i4]))\n",
        "    print()\n",
        "    \n",
        "    val_loss_ = validate().numpy()\n",
        "    history['val'].append((epoch,val_loss_))\n",
        "    print ('\\n* Train Loss {:.4f}'.format(train_loss.result()))\n",
        "    history['train'].append((epoch,train_loss.result().numpy()))\n",
        "    \n",
        "    \n",
        "    if best_val_loss-val_loss_ > 0.1:\n",
        "        ckpt_save_path4 = ckpt_manager4.save()\n",
        "        print ('\\nSaving checkpoint for epoch {} at {}'.format(epoch, ckpt_save_path4))  \n",
        "        best_val_loss = val_loss_\n",
        "    \n",
        "    hist(history)\n",
        "    print('Current Lr: ',optimizer2._decayed_lr('float32').numpy())\n",
        "    print ('\\nTime taken for this epoch: {:.2f} secs\\n'.format(time.time() - start))\n",
        "    print('='*40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'lmbda_l2': 0.1, 'd_out_rate': 0.1, 'num_layers': 4, 'd_model': 300, 'dff': 512, 'num_heads': 5, 'init_lr': 0.001}\n",
            "########################################\n",
            "Epoch 1 Batch 0 Loss 1.7646\n",
            "\n",
            "\n",
            "validation started ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-503-f626c2b71eca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mval_loss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'\\n* Train Loss {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-498-b775a521d690>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mcombined_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mdec_padding_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         )\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-479-de5e42188ff1>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_ahead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-478-c990e1efbc8b>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, enc_output, training, look_ahead_mask, padding_mask)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    194\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m       \u001b[0;31m# Instead of casting the variable as in most layers, cast the output, as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup_v2\u001b[0;34m(params, ids, max_norm, name)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m   \"\"\"\n\u001b[0;32m--> 394\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[0;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[1;32m    326\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m       transform_fn=None)\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[0;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[1;32m    136\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         result = _clip(\n\u001b[0;32m--> 138\u001b[0;31m             array_ops.gather(params[0], ids, name=name), ids, max_norm)\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4813\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4814\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4815\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   3786\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3787\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3788\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3789\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3790\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[0,6] = 342 is not in [0, 42) [Op:GatherV2]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxbr5KnMQWAe"
      },
      "source": [
        "# for epoch in range(EPOCHS):\r\n",
        "#     start = time.time()\r\n",
        "\r\n",
        "#     train_loss.reset_states()\r\n",
        "  \r\n",
        "#     for (batch, (inp, tar)) in enumerate(dataset):\r\n",
        "#         train_step(inp, tar)\r\n",
        "    \r\n",
        "#         # 55k samples\r\n",
        "#         # we display 3 batch results -- 0th, middle and last one (approx)\r\n",
        "#         # 55k / 64 ~ 858; 858 / 2 = 429\r\n",
        "#         if batch % 429 == 0:\r\n",
        "#             print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\r\n",
        "      \r\n",
        "#     # if (epoch + 1) % 5 == 0:\r\n",
        "#     #     ckpt_save_path = ckpt_manager.save()\r\n",
        "#     #     print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\r\n",
        "    \r\n",
        "#     print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\r\n",
        "\r\n",
        "#     print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDvi6IVqkfKU"
      },
      "source": [
        "hist(history)\n",
        "params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNVOWPXFIn0k"
      },
      "source": [
        "print(document_val[i1])\n",
        "print()\n",
        "print(summarize(document_val[i1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za9AcM1JRPrC"
      },
      "source": [
        "summarize(\r\n",
        "    \"तराईमा आएको बाढीका बाबजुत यो वर्ष हालसम्मकै उच्च परिमाणमा धान उत्पादन हुने भएको छ। कृषि विकास मन्त्रालयले यो वर्षको धान उत्पादनले विगतका सबै इतिहास तोड्ने दावी गरेको छ। मन्त्रालयका सहप्रवक्ता शंकर सापकोटाले यो वर्ष हालसम्मकै धेरै धान उत्पादन हुने प्रक्षेपण गरिएको बताए।\"\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}